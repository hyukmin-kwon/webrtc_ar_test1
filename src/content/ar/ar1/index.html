<!DOCTYPE html>
<!--
 *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
 *
 *  Use of this source code is governed by a BSD-style license
 *  that can be found in the LICENSE file in the root of the source
 *  tree.
-->
<html>
<head>

  <meta charset="utf-8">
  <meta name="description" content="WebRTC code samples">
  <meta name="viewport" content="width=device-width, user-scalable=yes, initial-scale=1, maximum-scale=1">
  <meta itemprop="description" content="Client-side WebRTC code samples">
  <meta itemprop="image" content="../../../images/webrtc-icon-192x192.png">
  <meta itemprop="name" content="WebRTC code samples">
  <meta name="mobile-web-app-capable" content="yes">
  <meta id="theme-color" name="theme-color" content="#ffffff">

  <base target="_blank">

  <title>Select audio and video sources</title>

  <link rel="icon" sizes="192x192" href="../../../images/webrtc-icon-192x192.png">
  <link href="//fonts.googleapis.com/css?family=Roboto:300,400,500,700" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="../../../css/main.css">

  <style>
    div.select {
      display: inline-block;
      margin: 0 0 1em 0;
    }
    p.small {
      font-size: 0.7em;
    }
    label {
      width: 12em;
      display: inline-block;
    }
  </style>

</head>

<body>
  <div id="container">

    <div class="highlight">
      <p>New codelab: <a href="https://codelabs.developers.google.com/codelabs/webrtc-web">Realtime communication with WebRTC</a></p>
    </div>

    <h1><a href="//webrtc.github.io/samples/" title="WebRTC samples homepage">WebRTC samples</a><span>Select sources &amp; outputs</span></h1>

    <p>Get available audio, video sources and audio output devices<b>*</b> from <code>mediaDevices.enumerateDevices()</code> then set the source for <code>getUserMedia()</code> using a <code>deviceId</code> constraint.</p>

    <p class="small"><b>*</b>Enable experimental support in <b>Chrome 45.0.2441.x</b> or later by selecting <b>Enable experimental Web Platform features</b> in <b>chrome://flags</b> or by using command line flag "<b>--enable-blink-features=EnumerateDevices,AudioOutputDevices</b>". Use the <code>setSinkID()</code> method on the video element and provide a audio or video element and a sinkId (<code>deviceId</code> for where <code>deviceInfo.kind === 'audiooutput'</code>) as arguments. Also the web page must be served over HTTPS.</p>

    <div class="select">
      <label for="audioSource">Audio input source: </label><select id="audioSource"></select>
    </div>

    <div class="select">
      <label for="audioOutput">Audio output destination: </label><select id="audioOutput"></select>
    </div>

    <div class="select">
      <label for="videoSource">Video source: </label><select id="videoSource"></select>
    </div>

    <video id="video" autoplay></video>

    <p class="small"><b>Note:</b> If you hear a reverb sound your microphone is picking up the output of your speakers/headset, lower the volume and/or move the microphone further away from your speakers/headset.</p>

    <a href="https://github.com/webrtc/samples/tree/gh-pages/src/content/devices/input-output" title="View source for this page on GitHub" id="viewSource">View source on GitHub</a>
  </div>

  <!--  From JSARToolkit begin -->
  <script src="js/third_party/three.js/three.min.js"></script>
  <script src="../build/artoolkit.debug.js"></script>
  <script src="../js/artoolkit.api.js"></script>
  <script id="vert" type="glsl-vertex">
    precision highp float;
    precision lowp int;

    uniform mat4 cameraMatrix;
    uniform mat4 transformationMatrix;

    varying vec2 vUv;

    void main(void)
    {
        vUv = uv;
        gl_Position = cameraMatrix * transformationMatrix * vec4(position, 1.0);
    }
    </script>

    <script id="frag" type="glsl-fragment">
    precision highp float;
    precision lowp int;

    varying vec2 vUv;

    void main(void)
    {
        gl_FragColor = vec4(vUv, 1.0, 1.0);
    }
    </script>

  <!--  From JSARToolkit end -->

  <script src="../../../js/adapter.js"></script>
  <script src="../../../js/common.js"></script>
  <script src="js/main.js"></script>

  <!--  From JSARToolkit begin -->
<script>

var cMat = new THREE.Matrix4();
var tMat = new THREE.Matrix4();

var USE_SHADER = true;

var shaderMaterial = new THREE.ShaderMaterial({
	uniforms: {
		cameraMatrix: {type: 'm4', value: cMat },
		transformationMatrix: {type: 'm4', value: tMat }
	},
	vertexShader: vert.text,
	fragmentShader: frag.text
});


var renderer = new THREE.WebGLRenderer();
var scene = new THREE.Scene();

var video = document.getElementById('video');
renderer.setSize(video.width, video.height);

document.body.appendChild(renderer.domElement);

// Create a camera and a marker root object for your Three.js scene.
var camera = new THREE.Camera();
scene.add(camera);

var light = new THREE.PointLight(0xffffff);
light.position.set(400, 500, 100);
scene.add(light);
var light = new THREE.PointLight(0xffffff);
light.position.set(-400, -500, -100);
scene.add(light);

var markerRoot = new THREE.Object3D();

markerRoot.wasVisible = false;
markerRoot.markerMatrix = new Float64Array(12);
markerRoot.matrixAutoUpdate = false;
camera.matrixAutoUpdate = false;

// Add the marker models and suchlike into your marker root object.

var cube = new THREE.Mesh(
	new THREE.BoxGeometry(1,1,1),
	USE_SHADER ?
		shaderMaterial :
		new THREE.MeshLambertMaterial({ color: 0xffffff, wireframe: false })
);
markerRoot.add(cube);

// Add the marker root to your scene.
scene.add(markerRoot);
var arController = null;



// On every frame do the following:
function tick() {
	requestAnimationFrame(tick);

	if (!arController) {
		return;
	}

	arController.detectMarker(video);
	var markerNum = arController.getMarkerNum();
	if (markerNum > 0) {
		if (markerRoot.visible) {
			arController.getTransMatSquareCont(0, 1, markerRoot.markerMatrix, markerRoot.markerMatrix);
		} else {
			arController.getTransMatSquare(0 /* Marker index */, 1 /* Marker width */, markerRoot.markerMatrix);
		}
		markerRoot.visible = true;
		if (USE_SHADER) {
			arController.transMatToGLMat(markerRoot.markerMatrix, shaderMaterial.uniforms.transformationMatrix.value.elements);
		} else {
			arController.transMatToGLMat(markerRoot.markerMatrix, markerRoot.matrix.elements);
		}
	} else {
		markerRoot.visible = false;
	}

	arController.debugDraw();

	// Render the scene.
	renderer.autoClear = false;
	renderer.clear();
	renderer.render(scene, camera);
}

tick();

var cameraParam = new ARCameraParam();
cameraParam.onload = function() {

	arController = new ARController(320, 240, cameraParam);
	arController.debugSetup();

	var camera_mat = arController.getCameraMatrix();

	if (USE_SHADER) {
		shaderMaterial.uniforms.cameraMatrix.value.elements.set(camera_mat);
	} else {
		camera.projectionMatrix.elements.set(camera_mat);
	}

};
cameraParam.load('Data/camera_para.dat');


</script>

  <!--  From JSARToolkit end -->

</body>
</html>
